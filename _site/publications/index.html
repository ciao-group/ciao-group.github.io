<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!--  <title>Publications</title> -->
  <title>Publications - CIAO Group</title>
  <meta name="description" content="Focusing on the nexus of machine learning and human-computer interaction, SCADS CIAO Group models human-like behaviors in the context of automated vehicular technology.">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">
  <link rel="canonical" href="http://localhost:4000/publications/">
<link rel="shortcut icon" type ="image/x-icon" href="http://localhost:4000/favicon.ico">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>
      <a class="navbar-brand" href="http://localhost:4000/">CIAO Group</a>

	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="http://localhost:4000/">Home</a></li>
        
          <li><a href="http://localhost:4000/publications">Publications</a></li>
        
          <li><a href="http://localhost:4000/research">Research</a></li>
        
          <li><a href="http://localhost:4000/team">Team</a></li>
        
	  </ul>
	</div>
  </div>
</div>


    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12">
  <h1 id="publications">Publications</h1>

<h2 id="2025">2025</h2>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/Paper5.png" class="img-responsive" width="90%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> Exploring Millions of User Interactions with ICEBOAT: Big Data Analytics for Automotive User Interfaces</strong> <br />
  <em>Patrick Ebel, Kim Julian Gülle, Christoph Lingenfeld, Andreas Vogelsang </em><br />
  on Automotive User Interfaces and Interactive Vehicular Applications (Auto- motiveUI, 23), (2023)<br />
  <a href="http://localhost:4000/papers/Paper5.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  <a href="http://dx.doi.org/10.1145/3580585.3607158" target="_blank"><button class="btn-doi">DOI</button></a> 
  
  
   <a data-toggle="collapse" href="#Paper5" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="Paper5">ABSTRACT</a>


<br />
<div class="collapse" id="Paper5"><div class="well-abstract">
 User Experience (UX) professionals need to be able to analyze large amounts of usage data on their own to make evidence-based design decisions. However, the design process for In-Vehicle Information Systems (IVISs) lacks data-driven support and effective tools for visualizing and analyzing user interaction data. Therefore, we pro- pose ICEBOAT1, an interactive visualization tool tailored to the needs of automotive UX experts to effectively and efficiently evalu- ate driver interactions with IVISs. ICEBOAT visualizes telematics data collected from production line vehicles, allowing UX experts to perform task-specific analyses. Following a mixed methods User- Centered Design (UCD) approach, we conducted an interview study (N=4) to extract the domain specific information and interaction needs of automotive UX experts and used a co-design approach (N=4) to develop an interactive analysis tool. Our evaluation (N=12) shows that ICEBOAT enables UX experts to efficiently generate knowledge that facilitates data-driven design decisions.
</div></div>




</li>
</ul>

</div>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/Paper9.png" class="img-responsive" width="90%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> Multitasking While Driving: How Drivers Self- Regulate Their Interaction with In-Vehicle Touchscreens in Automated Driving</strong> <br />
  <em>Patrick Ebel, Christoph Lingenfeld, Andreas Vogelsang </em><br />
  International Journal of Human-Computer Interaction (2023)<br />
  <a href="http://localhost:4000/papers/Paper9.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  <a href="http://dx.doi.org/10.1080/10447318.2023.2215634" target="_blank"><button class="btn-doi">DOI</button></a> 
  
  
   <a data-toggle="collapse" href="#Paper9" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="Paper9">ABSTRACT</a>


<br />
<div class="collapse" id="Paper9"><div class="well-abstract">
 With modern infotainment systems, drivers are increasingly tempted to engage in secondary tasks while driving. Since distracted driving is already one of the main causes of fatal accidents, in-vehicle touchscreens must be as little distracting as possible. To ensure that these systems are safe to use, they undergo elaborate and expensive empirical testing, requiring fully functional prototypes. Thus, early-stage methods informing designers about the implication their design may have on driver distraction are of great value. This paper presents a machine learning method that, based on anticipated usage scenarios, predicts the visual demand of in-vehicle touchscreen interactions and provides local and global explanations of the factors influencing drivers’ visual attention allocation. The approach is based on large-scale natural driving data continuously collected from production line vehicles and employs the SHapley Additive exPlanation (SHAP) method to provide explanations leveraging informed design decisions. Our approach is more accurate than related work and identifies interactions during which long glances occur with 68 % accuracy and predicts the total glance duration with a mean error of 2.4s. Our explanations replicate the results of various recent studies and provide fast and easily accessible insights into the effect of UI elements, driving automation, and vehicle speed on driver distraction. The system can not only help designers to evaluate current designs but also help them to better anticipate and understand the implications their design decisions might have on future designs.
</div></div>




</li>
</ul>

</div>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/Paper3.png" class="img-responsive" width="90%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> Data-Driven Evaluation of In-Vehicle Information Systems</strong> <br />
  <em>Patrick Ebel </em><br />
  Dissertation (2023)<br />
  <a href="http://localhost:4000/papers/Paper3.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  
  
  
   <a data-toggle="collapse" href="#Paper3" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="Paper3">ABSTRACT</a>


<br />
<div class="collapse" id="Paper3"><div class="well-abstract">
 Today’s In-Vehicle Information Systems (IVISs) are feature-rich systems that provide the driver with numerous options for entertainment, information, comfort, and commu- nication. Drivers can stream their favorite songs, read reviews of nearby restaurants, or change the ambient lighting to their liking. To do so, they interact with large center stack touchscreens that have become the main interface between the driver and IVISs. To interact with these systems, drivers must take their eyes off the road which can im- pair their driving performance. This makes IVIS evaluation critical not only to meet customer needs but also to ensure road safety. The growing number of features, the dis- traction caused by large touchscreens, and the impact of driving automation on driver behavior pose significant challenges for the design and evaluation of IVISs. Tradition- ally, IVISs are evaluated qualitatively or through small-scale user studies using driving simulators. However, these methods are not scalable to the growing number of features and the variety of driving scenarios that influence driver interaction behavior. We ar- gue that data-driven methods can be a viable solution to these challenges and can assist automotive User Experience (UX) experts in evaluating IVISs. Therefore, we need to understand how data-driven methods can facilitate the design and evaluation of IVISs, how large amounts of usage data need to be visualized, and how drivers allocate their visual attention when interacting with center stack touchscreens. In Part I, we present the results of two empirical studies and create a comprehensive understanding of the role that data-driven methods currently play in the automotive UX design process. We found that automotive UX experts face two main conflicts: First, results from qualitative or small-scale empirical studies are often not valued in the decision-making process. Second, UX experts often do not have access to customer data and lack the means and tools to analyze it appropriately. As a result, design decisions are often not user-centered and are based on subjective judgments rather than evidence-based customer insights. Our results show that automotive UX experts need data-driven methods that leverage large amounts of telematics data collected from customer vehicles. They need tools to help them visualize and analyze customer usage data and computational methods to automatically evaluate IVIS designs. In Part II, we present ICEBOAT, an interactive user behavior analysis tool for auto- motive user interfaces. ICEBOAT processes interaction data, driving data, and glance data, collected over-the-air from customer vehicles and visualizes it on different levels of granularity. Leveraging our multi-level user behavior analysis framework, it enables UX experts to effectively and efficiently evaluate driver interactions with touchscreen-based IVISs concerning performance and safety-related metrics. In Part III, we investigate drivers’ multitasking behavior and visual attention alloca- tion when interacting with center stack touchscreens while driving. We present the first naturalistic driving study to assess drivers’ tactical and operational self-regulation with center stack touchscreens. Our results show significant differences in drivers’ interaction and glance behavior in response to different levels of driving automation, vehicle speed, and road curvature. During automated driving, drivers perform more interactions per touchscreen sequence and increase the time spent looking at the center stack touchscreen. These results emphasize the importance of context-dependent driver distraction assess- ment of driver interactions with IVISs. Motivated by this we present a machine learning- based approach to predict and explain the visual demand of in-vehicle touchscreen inter- iii actions based on customer data. By predicting the visual demand of yet unseen touch- screen interactions, our method lays the foundation for automated data-driven evalua- tion of early-stage IVIS prototypes. The local and global explanations provide additional insights into how design artifacts and driving context affect drivers’ glance behavior. Overall, this thesis identifies current shortcomings in the evaluation of IVISs and proposes novel solutions based on visual analytics and statistical and computational modeling that generate insights into driver interaction behavior and assist UX experts in making user-centered design decisions.
</div></div>




</li>
</ul>

</div>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/Paper2.png" class="img-responsive" width="90%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> On the forces of driver distraction: Explainable predictions for the visual demand of in-vehicle touchscreen interactions</strong> <br />
  <em>Patrick Ebel, Christoph Lingenfeld, Andreas Vogelsang </em><br />
   (2023)<br />
  <a href="http://localhost:4000/papers/Paper2.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  <a href="http://dx.doi.org/10.1016/j.aap.2023.106956" target="_blank"><button class="btn-doi">DOI</button></a> 
  
  
   <a data-toggle="collapse" href="#Paper2" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="Paper2">ABSTRACT</a>


<br />
<div class="collapse" id="Paper2"><div class="well-abstract">
 With modern infotainment systems, drivers are increasingly tempted to engage in secondary tasks while driving. Since distracted driving is already one of the main causes of fatal accidents, in-vehicle touchscreens must be as little distracting as possible. To ensure that these systems are safe to use, they undergo elaborate and expensive empirical testing, requiring fully functional prototypes. Thus, early-stage methods informing designers about the implication their design may have on driver distraction are of great value. This paper presents a machine learning method that, based on anticipated usage scenarios, predicts the visual demand of in-vehicle touchscreen interactions and provides local and global explanations of the factors influencing drivers’ visual attention allocation. The approach is based on large-scale natural driving data continuously collected from production line vehicles and employs the SHapley Additive exPlanation (SHAP) method to provide explanations leveraging informed design decisions. Our approach is more accurate than related work and identifies interactions during which long glances occur with 68 % accuracy and predicts the total glance duration with a mean error of 2.4s. Our explanations replicate the results of various recent studies and provide fast and easily accessible insights into the effect of UI elements, driving automation, and vehicle speed on driver distraction. The system can not only help designers to evaluate current designs but also help them to better anticipate and understand the implications their design decisions might have on future designs.
</div></div>




</li>
</ul>

</div>

<h2 id="2022">2022</h2>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/Paper6.png" class="img-responsive" width="90%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> How Do Drivers Self-Regulate their Secondary Task Engagements? The Effect of Driving Automation on Touchscreen Interactions and Glance Behavior</strong> <br />
  <em>Patrick Ebel, Moritz Berger, Christoph Lingenfeld, Andreas Vogelsang </em><br />
  4th International Conference on Automotive User Interfaces and Interactive Vehicular Applications (AutomotiveUI 22) (2022)<br />
  <a href="http://localhost:4000/papers/Paper6.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  <a href="http://dx.doi.org/10.1145/3543174.3545173" target="_blank"><button class="btn-doi">DOI</button></a> 
  
  
   <a data-toggle="collapse" href="#Paper6" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="Paper6">ABSTRACT</a>


<br />
<div class="collapse" id="Paper6"><div class="well-abstract">
 With ever-improving driver assistance systems and large touch- screens becoming the main in-vehicle interface, drivers are more tempted than ever to engage in distracting non-driving-related tasks. However, little research exists on how driving automation affects drivers’ self-regulation when interacting with center stack touchscreens. To investigate this, we employ multilevel models on a real-world driving dataset consisting of 10,139 sequences. Our re- sults show significant differences in drivers’ interaction and glance behavior in response to varying levels of driving automation, vehi- cle speed, and road curvature. During partially automated driving, drivers are not only more likely to engage in secondary touchscreen tasks, but their mean glance duration toward the touchscreen also increases by 12 % (Level 1) and 20 % (Level 2) compared to manual driving. We further show that the effect of driving automation on drivers’ self-regulation is larger than that of vehicle speed and road curvature. The derived knowledge can facilitate the safety evalua- tion of infotainment systems and the development of context-aware driver monitoring systems.
</div></div>




</li>
</ul>

</div>

<h2 id="2021">2021</h2>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/Paper10.png" class="img-responsive" width="90%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> Automotive UX design and data-driven development: Narrowing the gap to support practitioners</strong> <br />
  <em>Patrick Ebel, Julia Orlovska, Sebastian Hünemeyer, Casper Wickman, Andreas Vogelsang, Rikard Soederberg </em><br />
  Transportation Research Interdisciplinary Perspectives (2021)<br />
  <a href="http://localhost:4000/papers/Paper10.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  
  
  
   <a data-toggle="collapse" href="#Paper10" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="Paper10">ABSTRACT</a>


<br />
<div class="collapse" id="Paper10"><div class="well-abstract">
 The development and evaluation of In-Vehicle Information Systems (IVISs) is strongly based on insights from qualitative studies conducted in artificial contexts (e.g., driving simulators or lab experiments). However, the growing complexity of the systems and the uncertainty about the context in which they are used, create a need to augment qualitative data with quantitative data, collected during real-world driving. In contrast to many digital companies that are already successfully using data-driven methods, Original Equipment Manufacturers (OEMs) are not yet succeeding in releasing the potentials such methods offer. We aim to understand what prevents automotive OEMs from applying data-driven methods, what needs practitioners formulate, and how collecting and analyzing usage data from vehicles can enhance UX activities. We adopted a Multiphase Mixed Methods approach comprising two interview studies with more than 15 UX practitioners and two action research studies conducted with two different OEMs. From the four studies, we synthesize the needs of UX designers, extract limitations within the domain that hinder the application of data-driven methods, elaborate on unleveraged potentials, and formulate recommendations to improve the usage of vehicle data. We conclude that, in addition to modernizing the legal, technical, and organizational infrastructure, UX and Data Science must be brought closer together by reducing silo mentality and increasing interdisciplinary collaboration. New tools and methods need to be developed and UX experts must be empowered to make data-based evidence an integral part of the UX design process.
</div></div>




</li>
</ul>

</div>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/Paper7.png" class="img-responsive" width="90%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> Measuring Interaction-based Secondary Task Load: A Large-Scale Approach using Real-World Driving Data</strong> <br />
  <em>Patrick Ebel, Christoph Lingenfeld, Andreas Vogelsang </em><br />
   (2021)<br />
  <a href="http://localhost:4000/papers/Paper7.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  <a href="http://dx.doi.org/10.1145/3473682.3480252" target="_blank"><button class="btn-doi">DOI</button></a> 
  
  
   <a data-toggle="collapse" href="#Paper7" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="Paper7">ABSTRACT</a>


<br />
<div class="collapse" id="Paper7"><div class="well-abstract">
 With ever-improving driver assistance systems and large touch- screens becoming the main in-vehicle interface, drivers are more tempted than ever to engage in distracting non-driving-related tasks. However, little research exists on how driving automation affects drivers’ self-regulation when interacting with center stack touchscreens. To investigate this, we employ multilevel models on a real-world driving dataset consisting of 10,139 sequences. Our re- sults show significant differences in drivers’ interaction and glance behavior in response to varying levels of driving automation, vehi- cle speed, and road curvature. During partially automated driving, drivers are not only more likely to engage in secondary touchscreen tasks, but their mean glance duration toward the touchscreen also increases by 12 % (Level 1) and 20 % (Level 2) compared to manual driving. We further show that the effect of driving automation on drivers’ self-regulation is larger than that of vehicle speed and road curvature. The derived knowledge can facilitate the safety evalua- tion of infotainment systems and the development of context-aware driver monitoring systems.
</div></div>




</li>
</ul>

</div>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/Paper8.png" class="img-responsive" width="90%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> Visualizing Event Sequence Data for User Behavior Evaluation of In-Vehicle Information Systems</strong> <br />
  <em>Patrick Ebel, Christoph Lingenfeld, Andreas Vogelsang </em><br />
  13th International Conference on Automotive User Interfaces and Interactive Vehicular Applications (AutomotiveUI ’21), (2021)<br />
  <a href="http://localhost:4000/papers/Paper8.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  <a href="http://dx.doi.org/10.1145/3409118.3475140" target="_blank"><button class="btn-doi">DOI</button></a> 
  
  
   <a data-toggle="collapse" href="#Paper8" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="Paper8">ABSTRACT</a>


<br />
<div class="collapse" id="Paper8"><div class="well-abstract">
 With modern In-Vehicle Information Systems (IVISs) becoming more capable and complex than ever, their evaluation becomes in- creasingly difficult. The analysis of large amounts of user behavior data can help to cope with this complexity and can support UX experts in designing IVISs that serve customer needs and are safe to operate while driving. We, therefore, propose a Multi-level User Behavior Visualization Framework providing effective visualiza- tions of user behavior data that is collected via telematics from production vehicles. Our approach visualizes user behavior data on three different levels: (1) The Task Level View aggregates event sequence data generated through touchscreen interactions to vi- sualize user flows. (2) The Flow Level View allows comparing the individual flows based on a chosen metric. (3) The Sequence Level View provides detailed insights into touch interactions, glance, and driving behavior. Our case study proves that UX experts consider our approach a useful addition to their design process.
</div></div>




</li>
</ul>

</div>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/Paper1.png" class="img-responsive" width="90%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> The Role and Potentials of Field User Interaction Data in the Automotive UX Development Lifecycle: An Industry Perspective</strong> <br />
  <em>Patrick Ebel, Christoph Lingenfeld, Andreas Vogelsang </em><br />
   (2021)<br />
  <a href="http://localhost:4000/papers/Paper1.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  <a href="http://dx.doi.org/10.1145/3409120.3410638" target="_blank"><button class="btn-doi">DOI</button></a> 
  
  
   <a data-toggle="collapse" href="#Paper1" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="Paper1">ABSTRACT</a>


<br />
<div class="collapse" id="Paper1"><div class="well-abstract">
 We are interested in the role of field user interaction data in the de- velopment of In-Vehicle Information Systems (IVISs), the potentials practitioners see in analyzing this data, the concerns they share, and how this compares to companies with digital products. We conducted interviews with 14 UX professionals, 8 from automotive and 6 from digital companies, and analyzed the results by emergent thematic coding. Our key findings indicate that implicit feedback through field user interaction data is currently not evident in the automotive UX development process. Most decisions regarding the design of IVISs are made based on personal preferences and the intuitions of stakeholders. However, the interviewees also indicated that user interaction data has the potential to lower the influence of guesswork and assumptions in the UX design process and can help to make the UX development lifecycle more evidence-based and user-centered.
</div></div>




</li>
</ul>

</div>

<h2 id="2020">2020</h2>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/Paper4.png" class="img-responsive" width="90%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> Destination Prediction Based on Partial Trajectory Data</strong> <br />
  <em>Patrick Ebel, Ibrahim Emre Göl, Christoph Lingenfelder, Andreas Vogelsang </em><br />
  IEEE Intelligent Vehicles Symposium (2020)<br />
  <a href="http://localhost:4000/papers/Paper4.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  
  <a href="https://arxiv.org/abs/2004.07473" target="_blank"><button class="btn-arxiv">ARXIV</button></a> 
  
   <a data-toggle="collapse" href="#Paper4" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="Paper4">ABSTRACT</a>


<br />
<div class="collapse" id="Paper4"><div class="well-abstract">
 Two-thirds of the people who buy a new car prefer to use a substitute instead of the built-in navigation system. However, for many applications, knowledge about a users intended destination and route is crucial. For example, suggestions for available parking spots close to the destination can be made or ride-sharing opportunities along the route are facilitated. Our approach predicts probable destinations and routes of a vehicle, based on the most recent partial trajectory and additional contextual data. The approach follows a three- step procedure: First, a k-d tree-based space discretization is performed, mapping GPS locations to discrete regions. Secondly, a recurrent neural network is trained to predict the destination based on partial sequences of trajectories. The neural network produces destination scores, signifying the prob- ability of each region being the destination. Finally, the routes to the most probable destinations are calculated. To evaluate the method, we compare multiple neural architectures and present the experimental results of the destination prediction. The experiments are based on two public datasets of non- personalized, timestamped GPS locations of taxi trips. The best performing models were able to predict the destination of a vehicle with a mean error of 1.3 km and 1.43 km respectively.
</div></div>




</li>
</ul>

</div>


</div>

      </div>
    </div>

    <!--
<div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-4">
			
            <p>&copy 2023 CIAO Group </p>

             <p></p> 

		   
            		  
		</div>

		<div class="col-sm-4">
            <p></p> 
		 
	
		<center>
	   <a href="https://scads.ai" target="_blank"><i class="fa fa-home fa-2x"></i></a> 
	    <a href="https://github.com/ciao-group" target="_blank"><i class="fa fa-github-square fa-2x"></i></a> 
	    <a href="https://scholar.google.com/citations?user=nRW4gQQAAAAJ&hl=en" target="_blank"><i class="ai ai-google-scholar-square ai-2x"></i></a> 	   
	    </center>
		
	
	
	
		</div>


		<div class="col-sm-4">
            <p>Löhrs Carré <br/> Humboldtstraße 25 <br/> 3. Obergeschoss <br/> Room D03.31 <br/> 04105 Leipzig <br/>
 
</p>
		</div>

	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="http://localhost:4000/js/bootstrap.min.js"></script>

-->

<div id="footer" class="panel">
	<div class="panel-footer">
	  <div class="container-fluid">
		<div class="row">


		  <!-- Site Title Column -->
		  <div class="col-sm-4">
			<p>&copy 2023 CIAO Group</p>
		  </div>
  
		  <!-- Member Icons Column -->
		  <div class="col-sm-4" style="display: flex; justify-content: center;">
			<div style="margin-top: 16px;"> <!-- Adjust 'margin-top' to push icons lower -->
			  

			  	
			  	<a href="https://www.uni-leipzig.de/en/profile/mitarbeiter/patrick-ebel" target="_blank">
				<i class="fa fa-envelope-square fa-2x"></i>
			  	</a>
				
			 	
				  <a href="https://scads.ai" target="_blank">
					<i class="fa fa-home fa-2x"></i>
				  </a>
				
				
				  <a href="https://github.com/ciao-group" target="_blank">
					<i class="fa fa-github-square fa-2x"></i>
				  </a>
				
				
				  <a href="https://scholar.google.com/citations?user=nRW4gQQAAAAJ&hl=en" target="_blank">
					<i class="ai ai-google-scholar-square ai-2x"></i>
				  </a>
				
			  
			</div>
		  </div>
  
		  <!-- Site Location Column -->
		  <div class="col-sm-4" style="text-align: right;">
			<p>Löhrs Carré <br/> Humboldtstraße 25 <br/> 3. Obergeschoss <br/> Room D03.31 <br/> 04105 Leipzig <br/>
 
</p>
		  </div>
		</div>
	  </div>
	</div>
  </div>
  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="http://localhost:4000/js/bootstrap.min.js"></script>
  

	
	



  </body>

</html>
