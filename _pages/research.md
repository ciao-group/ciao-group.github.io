---
title: "Research"
layout: gridlay
sitemap: false
permalink: /research/
---
<style>
code {padding: 6px 8px; font-size: 90%;}
.row h2 {padding: 16px 16px}
.row p {padding: 16px 16px}
.rowl1 p {padding: 0px 0px}
.rowl1 img {padding: 16px 16px}
.rowl2 p {padding: 16px 16px}
.rowl2 h4 {padding: 16px 16px}
.rowl2 img {padding: 16px 16px}

</style>

# Research

<div class="row">
  
  <p> Advances in AI are reshaping the interaction between humans and intelligent machines.
As these systems become more complex, it is increasingly important to design them in alignment with human goals and intentions. However, the growing complexity and fast-paced development renders the traditional user-centered design approach unsustainable. Experimental user testing and qualitative studies are often overlooked due to their slow, resource-intensive nature. To address this, our research uses data-driven analytics and machine learning-based user modeling and simulation to make user-centered design more efficient and to better understand how humans interact with intelligent computing technology. </p>


<div class="rowl1">
  <img src="{{ site.url }}{{ site.baseurl }}/images/respic/CompRationalCar.png" class="img-responsive" width="50%" style=" float: left; border-radius:7px" />
  <h4 style="overflow: hidden"> Computational Rational User Modeling </h4>
  <p style="overflow: hidden"> Computational rational theories assume that users choose their behavior to maximize their expected utility, given their bounds. Applied to human-computer interaction, users interact with technology so that they can achieve an optimal outcome given their internal (e.g., cognition or perception) and external (e.g., environment or design of a tool) bounds. Thus, users adapt their behavioral policies or strategies to maximize their utility. These policies can be approximated via reinforcement learning, yielding verifiable predictions of user interaction behavior. Although machine learning is used to learn behavioral policies and make predictions, the formulated bounds (e.g., perceptual or motor constraints) limit the space of computable interaction strategies of the agent so that the interactions represent realistic human behavior. </p>
</div>


<div class="rowl1">
  <img src="{{ site.url }}{{ site.baseurl }}/images/respic/biomech.png" class="img-responsive" width="50%" style=" float: right; border-radius:7px" />
  <h4 style="overflow: hidden"> Biomechanical Forward Simulation </h4>
  <p style="overflow: hidden"> Movement is the only way humans can influence the world. With the exception of brain–computer interfaces, all human communication is ultimately mediated through muscle contractions. Simulated users that replicate human-like, dexterous interactions with technology offer great potential to make user-centered design more efficient, rigorous, and predictable. By using forward simulation, these models produce biomechanically realistic motion, allowing accurate predictions of user performance and muscle effort based on musculoskeletal dynamics. Our work focuses on building biomechanical models that simulate human–computer interaction to inform and improve user interface design.
  </p>
</div>



<div class="rowl1">
  <img src="{{ site.url }}{{ site.baseurl }}/images/respic/DataDriven.png" class="img-responsive" width="50%" style=" float: left; border-radius:7px" />
  <h4 style="overflow: hidden; padding: 0px 16px"> Data-Driven Driver Modeling </h4>
  <p style="padding: 0px 16px"> The goal of CIAOs research is to better understand how drivers allocate their resources while engaging in secondary tasks. A deep understanding of human multitasking and how distraction affects driving behavior and vice versa facilitates the design of safe automotive HMIs. We combine large-scale user interaction data with driving data and glance behavior data to model driver's distraction, behavioral adaption, and sensitivity to the driving context. While we apply statistical modeling to evaluate new design artifacts, we also apply various machine learning approaches to predict human interaction behavior to not only better understand the interaction itself but also to evaluate new interfaces already before the first user study is run. </p>
</div>



<div class="rowl1">
  <img src="{{ site.url }}{{ site.baseurl }}/images/respic/simulator.JPG" class="img-responsive" width="50%" style=" float: right; border-radius:7px" />
  <h4 style="overflow: hidden"> Mixed-Reality Driving Simulator </h4>
  <p style="overflow: hidden"> To evaluate the computational models with human participants we to aim mimic the real driving environment as closely as possible. Our mixed-reality driving simulator allows us to create an immersive driving and interaction experience. We exploit the immersiveness of virtual reality to simulate the driving environment but keep the vehicle interior real such that secondary task engagements (e.g., interactions with the center stack touchscreen) feel as real as possible. The simulator's code is open source, and we welcome anyone who wants to use it or collaborate with us: <a href="https://github.com/ciao-group/coupled-sim-ciao"> GitHub Repository</a> </p>
</div>

